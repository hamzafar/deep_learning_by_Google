{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seed random value for reproduciablit\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data in dataframe\n",
    "df_train = pd.read_csv('D:/deepL_google/project/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_df = df_train\n",
    "print(len(tmp_df))\n",
    "tmp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into feature set and target values\n",
    "# x_train --> features\n",
    "# y_trian --> target\n",
    "x_train = np.array(tmp_df.ix[:,1:785])\n",
    "y_train = np.array(tmp_df.ix[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert target values to hot enconding\n",
    "# in this case we have 10 classes (0-9)\n",
    "y_labels = keras.utils.to_categorical(y_train, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Reshape data \n",
    "X_train = x_train.reshape(x_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_train = X_train/255.0\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc298f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[6,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplest Model with one conv layer and fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Convolutional layer with 10 filter and size of 5x5\n",
    "model.add(Conv2D(10, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "\n",
    "# fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 10, 24, 24)        260       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5760)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                57610     \n",
      "=================================================================\n",
      "Total params: 57,870.0\n",
      "Trainable params: 57,870.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 25s - loss: 1.5569 - acc: 0.6890 - val_loss: 0.7686 - val_acc: 0.8326\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.5387 - acc: 0.8579 - val_loss: 0.3877 - val_acc: 0.8913\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.3510 - acc: 0.8993 - val_loss: 0.3041 - val_acc: 0.9157\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.2912 - acc: 0.9170 - val_loss: 0.2674 - val_acc: 0.9239\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.2585 - acc: 0.9282 - val_loss: 0.2428 - val_acc: 0.9323\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.2333 - acc: 0.9359 - val_loss: 0.2191 - val_acc: 0.9393\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.2105 - acc: 0.9415 - val_loss: 0.2014 - val_acc: 0.9444\n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.1930 - acc: 0.9461 - val_loss: 0.1864 - val_acc: 0.9489\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.1767 - acc: 0.9508 - val_loss: 0.1737 - val_acc: 0.9530\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.1636 - acc: 0.9546 - val_loss: 0.1604 - val_acc: 0.9554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xce60eb8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn dl model\n",
    "model.fit(X_train, y_labels, epochs=10, batch_size=2000, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Dropout to above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Convolutional layer with 10 filter and size of 5x5\n",
    "model.add(Conv2D(10, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "\n",
    "# Dropout layer\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 10, 24, 24)        260       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 24, 24)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5760)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                57610     \n",
      "=================================================================\n",
      "Total params: 57,870.0\n",
      "Trainable params: 57,870.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 7s - loss: 1.5617 - acc: 0.6334 - val_loss: 0.7690 - val_acc: 0.8269\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.5444 - acc: 0.8563 - val_loss: 0.3890 - val_acc: 0.8883\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.3575 - acc: 0.8953 - val_loss: 0.3066 - val_acc: 0.9130\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.2994 - acc: 0.9141 - val_loss: 0.2682 - val_acc: 0.9249\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.2667 - acc: 0.9237 - val_loss: 0.2398 - val_acc: 0.9340\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.2406 - acc: 0.9316 - val_loss: 0.2178 - val_acc: 0.9398\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.2202 - acc: 0.9379 - val_loss: 0.2015 - val_acc: 0.9435\n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.2019 - acc: 0.9431 - val_loss: 0.1872 - val_acc: 0.9479\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.1862 - acc: 0.9479 - val_loss: 0.1767 - val_acc: 0.9507\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.1742 - acc: 0.9511 - val_loss: 0.1636 - val_acc: 0.9543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd28dac8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn dl model\n",
    "model.fit(X_train, y_labels, epochs=10, batch_size=2000, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding pooling layer to above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Convolutional layer with 10 filter and size of 5x5\n",
    "model.add(Conv2D(10, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "\n",
    "#Max pool layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Dropout layer\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 10, 24, 24)        260       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 10, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1440)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                14410     \n",
      "=================================================================\n",
      "Total params: 14,670.0\n",
      "Trainable params: 14,670.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 4s - loss: 1.9951 - acc: 0.4282 - val_loss: 1.5493 - val_acc: 0.7365\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 3s - loss: 1.1919 - acc: 0.7535 - val_loss: 0.8141 - val_acc: 0.8163\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.6781 - acc: 0.8221 - val_loss: 0.5113 - val_acc: 0.8660\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.4882 - acc: 0.8626 - val_loss: 0.3988 - val_acc: 0.8910\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.4077 - acc: 0.8840 - val_loss: 0.3480 - val_acc: 0.9051\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.3627 - acc: 0.8972 - val_loss: 0.3102 - val_acc: 0.9143\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.3317 - acc: 0.9057 - val_loss: 0.2864 - val_acc: 0.9217\n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.3068 - acc: 0.9123 - val_loss: 0.2644 - val_acc: 0.9271\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.2865 - acc: 0.9184 - val_loss: 0.2474 - val_acc: 0.9319\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 3s - loss: 0.2682 - acc: 0.9229 - val_loss: 0.2320 - val_acc: 0.9365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4bbc6ba8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn dl model\n",
    "model.fit(X_train, y_labels, epochs=10, batch_size=2000, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ia_model = Sequential()\n",
    "\n",
    "ia_model.add(Conv2D(50, (7, 7), input_shape=(1, 28, 28), activation='relu'))\n",
    "ia_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "ia_model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "ia_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "ia_model.add(Dropout(0.2))\n",
    "\n",
    "# fully connected layer\n",
    "ia_model.add(Flatten())\n",
    "\n",
    "ia_model.add(Dense(128, activation='relu'))\n",
    "ia_model.add(Dense(50, activation='relu'))\n",
    "\n",
    "ia_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "ia_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29399 samples, validate on 12601 samples\n",
      "Epoch 1/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0044 - acc: 0.9983 - val_loss: 0.0345 - val_acc: 0.9924\n",
      "Epoch 2/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0047 - acc: 0.9983 - val_loss: 0.0381 - val_acc: 0.9914\n",
      "Epoch 3/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0062 - acc: 0.9982 - val_loss: 0.0363 - val_acc: 0.9917\n",
      "Epoch 4/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0051 - acc: 0.9981 - val_loss: 0.0367 - val_acc: 0.9915\n",
      "Epoch 5/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0369 - val_acc: 0.9918\n",
      "Epoch 6/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0341 - val_acc: 0.9922\n",
      "Epoch 7/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0332 - val_acc: 0.9920\n",
      "Epoch 8/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0040 - acc: 0.9987 - val_loss: 0.0360 - val_acc: 0.9924\n",
      "Epoch 9/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0038 - acc: 0.9985 - val_loss: 0.0358 - val_acc: 0.9929\n",
      "Epoch 10/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0038 - acc: 0.9986 - val_loss: 0.0348 - val_acc: 0.9927\n",
      "Epoch 11/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0374 - val_acc: 0.9921\n",
      "Epoch 12/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0351 - val_acc: 0.9926\n",
      "Epoch 13/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0040 - acc: 0.9984 - val_loss: 0.0365 - val_acc: 0.9921\n",
      "Epoch 14/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0034 - acc: 0.9987 - val_loss: 0.0359 - val_acc: 0.9924\n",
      "Epoch 15/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0379 - val_acc: 0.9916\n",
      "Epoch 16/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0354 - val_acc: 0.9920\n",
      "Epoch 17/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0049 - acc: 0.9981 - val_loss: 0.0367 - val_acc: 0.9924\n",
      "Epoch 18/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0399 - val_acc: 0.9920\n",
      "Epoch 19/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0041 - acc: 0.9985 - val_loss: 0.0380 - val_acc: 0.9919\n",
      "Epoch 20/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0359 - val_acc: 0.9921\n",
      "Epoch 21/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0433 - val_acc: 0.9912\n",
      "Epoch 22/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0387 - val_acc: 0.9921\n",
      "Epoch 23/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0396 - val_acc: 0.9922\n",
      "Epoch 24/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0063 - acc: 0.9977 - val_loss: 0.0381 - val_acc: 0.9920\n",
      "Epoch 25/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0055 - acc: 0.9980 - val_loss: 0.0369 - val_acc: 0.9913\n",
      "Epoch 26/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0348 - val_acc: 0.9921\n",
      "Epoch 27/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0050 - acc: 0.9980 - val_loss: 0.0385 - val_acc: 0.9913\n",
      "Epoch 28/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0048 - acc: 0.9983 - val_loss: 0.0423 - val_acc: 0.9910\n",
      "Epoch 29/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0045 - acc: 0.9983 - val_loss: 0.0358 - val_acc: 0.9925\n",
      "Epoch 30/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0043 - acc: 0.9986 - val_loss: 0.0349 - val_acc: 0.9922\n",
      "Epoch 31/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0426 - val_acc: 0.9913\n",
      "Epoch 32/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0373 - val_acc: 0.9921\n",
      "Epoch 33/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0057 - acc: 0.9980 - val_loss: 0.0396 - val_acc: 0.9913\n",
      "Epoch 34/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0049 - acc: 0.9985 - val_loss: 0.0418 - val_acc: 0.9910\n",
      "Epoch 35/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0054 - acc: 0.9980 - val_loss: 0.0457 - val_acc: 0.9901\n",
      "Epoch 36/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0398 - val_acc: 0.9913\n",
      "Epoch 37/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0034 - acc: 0.9987 - val_loss: 0.0434 - val_acc: 0.9909\n",
      "Epoch 38/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0387 - val_acc: 0.9919\n",
      "Epoch 39/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0042 - acc: 0.9982 - val_loss: 0.0346 - val_acc: 0.9934\n",
      "Epoch 40/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0046 - acc: 0.9983 - val_loss: 0.0379 - val_acc: 0.9911\n",
      "Epoch 41/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0333 - val_acc: 0.9927\n",
      "Epoch 42/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0382 - val_acc: 0.9917\n",
      "Epoch 43/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0040 - acc: 0.9986 - val_loss: 0.0388 - val_acc: 0.9913\n",
      "Epoch 44/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0372 - val_acc: 0.9917\n",
      "Epoch 45/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0384 - val_acc: 0.9921\n",
      "Epoch 46/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0040 - acc: 0.9987 - val_loss: 0.0392 - val_acc: 0.9914\n",
      "Epoch 47/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0043 - acc: 0.9983 - val_loss: 0.0361 - val_acc: 0.9925\n",
      "Epoch 48/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0362 - val_acc: 0.9919\n",
      "Epoch 49/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0373 - val_acc: 0.9918\n",
      "Epoch 50/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0387 - val_acc: 0.9920\n",
      "Epoch 51/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0368 - val_acc: 0.9914\n",
      "Epoch 52/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0037 - acc: 0.9986 - val_loss: 0.0414 - val_acc: 0.9906\n",
      "Epoch 53/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0354 - val_acc: 0.9925\n",
      "Epoch 54/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0427 - val_acc: 0.9910\n",
      "Epoch 55/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0364 - val_acc: 0.9923\n",
      "Epoch 56/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0352 - val_acc: 0.9925\n",
      "Epoch 57/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0405 - val_acc: 0.9919\n",
      "Epoch 58/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0032 - acc: 0.9988 - val_loss: 0.0350 - val_acc: 0.9921\n",
      "Epoch 59/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0392 - val_acc: 0.9921\n",
      "Epoch 60/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0396 - val_acc: 0.9925\n",
      "Epoch 61/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0363 - val_acc: 0.9927\n",
      "Epoch 62/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0375 - val_acc: 0.9917\n",
      "Epoch 63/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0344 - val_acc: 0.9923\n",
      "Epoch 64/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0030 - acc: 0.9991 - val_loss: 0.0383 - val_acc: 0.9920\n",
      "Epoch 65/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0035 - acc: 0.9986 - val_loss: 0.0350 - val_acc: 0.9921\n",
      "Epoch 66/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0039 - acc: 0.9986 - val_loss: 0.0397 - val_acc: 0.9913\n",
      "Epoch 67/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0033 - acc: 0.9988 - val_loss: 0.0382 - val_acc: 0.9921\n",
      "Epoch 68/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0031 - acc: 0.9991 - val_loss: 0.0392 - val_acc: 0.9920\n",
      "Epoch 69/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0381 - val_acc: 0.9919\n",
      "Epoch 70/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0402 - val_acc: 0.9918\n",
      "Epoch 71/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0027 - acc: 0.9989 - val_loss: 0.0384 - val_acc: 0.9915\n",
      "Epoch 72/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0372 - val_acc: 0.9918\n",
      "Epoch 73/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0374 - val_acc: 0.9921\n",
      "Epoch 74/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0400 - val_acc: 0.9925\n",
      "Epoch 75/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0399 - val_acc: 0.9921\n",
      "Epoch 76/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0388 - val_acc: 0.9924\n",
      "Epoch 77/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0030 - acc: 0.9987 - val_loss: 0.0406 - val_acc: 0.9919\n",
      "Epoch 78/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0034 - acc: 0.9987 - val_loss: 0.0408 - val_acc: 0.9906\n",
      "Epoch 79/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0362 - val_acc: 0.9924\n",
      "Epoch 80/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0367 - val_acc: 0.9917\n",
      "Epoch 81/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0477 - val_acc: 0.9902\n",
      "Epoch 82/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0385 - val_acc: 0.9919\n",
      "Epoch 83/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0376 - val_acc: 0.9924\n",
      "Epoch 84/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0423 - val_acc: 0.9916\n",
      "Epoch 85/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0406 - val_acc: 0.9917\n",
      "Epoch 86/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0389 - val_acc: 0.9921\n",
      "Epoch 87/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0380 - val_acc: 0.9922\n",
      "Epoch 88/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0402 - val_acc: 0.9921\n",
      "Epoch 89/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0384 - val_acc: 0.9920\n",
      "Epoch 90/400\n",
      "29399/29399 [==============================] - 10s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0362 - val_acc: 0.9926\n",
      "Epoch 91/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0431 - val_acc: 0.9921\n",
      "Epoch 92/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0381 - val_acc: 0.9924\n",
      "Epoch 93/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0425 - val_acc: 0.9913\n",
      "Epoch 94/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0396 - val_acc: 0.9926\n",
      "Epoch 95/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0415 - val_acc: 0.9917\n",
      "Epoch 96/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0379 - val_acc: 0.9921\n",
      "Epoch 97/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0378 - val_acc: 0.9921\n",
      "Epoch 98/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0429 - val_acc: 0.9914\n",
      "Epoch 99/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0408 - val_acc: 0.9923\n",
      "Epoch 100/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0035 - acc: 0.9987 - val_loss: 0.0363 - val_acc: 0.9929\n",
      "Epoch 101/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0395 - val_acc: 0.9917\n",
      "Epoch 102/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0377 - val_acc: 0.9921\n",
      "Epoch 103/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0025 - acc: 0.9990 - val_loss: 0.0417 - val_acc: 0.9923\n",
      "Epoch 104/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0419 - val_acc: 0.9921\n",
      "Epoch 105/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0420 - val_acc: 0.9920\n",
      "Epoch 106/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0039 - acc: 0.9985 - val_loss: 0.0383 - val_acc: 0.9921\n",
      "Epoch 107/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0033 - acc: 0.9988 - val_loss: 0.0384 - val_acc: 0.9927\n",
      "Epoch 108/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0032 - acc: 0.9987 - val_loss: 0.0424 - val_acc: 0.9913\n",
      "Epoch 109/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0407 - val_acc: 0.9912\n",
      "Epoch 110/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0409 - val_acc: 0.9920\n",
      "Epoch 111/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0389 - val_acc: 0.9916\n",
      "Epoch 112/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0398 - val_acc: 0.9920\n",
      "Epoch 113/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0029 - acc: 0.9989 - val_loss: 0.0352 - val_acc: 0.9925\n",
      "Epoch 114/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0378 - val_acc: 0.9921\n",
      "Epoch 115/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0377 - val_acc: 0.9921\n",
      "Epoch 116/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0354 - val_acc: 0.9924\n",
      "Epoch 117/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0385 - val_acc: 0.9924\n",
      "Epoch 118/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0442 - val_acc: 0.9918\n",
      "Epoch 119/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0401 - val_acc: 0.9918\n",
      "Epoch 120/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0367 - val_acc: 0.9930\n",
      "Epoch 121/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0423 - val_acc: 0.9917\n",
      "Epoch 122/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0376 - val_acc: 0.9922\n",
      "Epoch 123/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0375 - val_acc: 0.9921\n",
      "Epoch 124/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0381 - val_acc: 0.9922\n",
      "Epoch 125/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0030 - acc: 0.9989 - val_loss: 0.0385 - val_acc: 0.9921\n",
      "Epoch 126/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0395 - val_acc: 0.9924\n",
      "Epoch 127/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0387 - val_acc: 0.9923\n",
      "Epoch 128/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0391 - val_acc: 0.9922\n",
      "Epoch 129/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0358 - val_acc: 0.9925\n",
      "Epoch 130/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0353 - val_acc: 0.9925\n",
      "Epoch 131/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0038 - acc: 0.9986 - val_loss: 0.0410 - val_acc: 0.9917\n",
      "Epoch 132/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0467 - val_acc: 0.9911\n",
      "Epoch 133/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0419 - val_acc: 0.9918\n",
      "Epoch 134/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0410 - val_acc: 0.9923\n",
      "Epoch 135/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0378 - val_acc: 0.9925\n",
      "Epoch 136/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0409 - val_acc: 0.9917\n",
      "Epoch 137/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0407 - val_acc: 0.9920\n",
      "Epoch 138/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0401 - val_acc: 0.9917\n",
      "Epoch 139/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0403 - val_acc: 0.9918\n",
      "Epoch 140/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0379 - val_acc: 0.9926\n",
      "Epoch 141/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0399 - val_acc: 0.9919\n",
      "Epoch 142/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0362 - val_acc: 0.9924\n",
      "Epoch 143/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0374 - val_acc: 0.9926\n",
      "Epoch 144/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0398 - val_acc: 0.9923\n",
      "Epoch 145/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0425 - val_acc: 0.9917\n",
      "Epoch 146/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0382 - val_acc: 0.9926\n",
      "Epoch 147/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0409 - val_acc: 0.9915\n",
      "Epoch 148/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0403 - val_acc: 0.9918\n",
      "Epoch 149/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0401 - val_acc: 0.9921\n",
      "Epoch 150/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0383 - val_acc: 0.9923\n",
      "Epoch 151/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0387 - val_acc: 0.9920\n",
      "Epoch 152/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0380 - val_acc: 0.9925\n",
      "Epoch 153/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0404 - val_acc: 0.9919\n",
      "Epoch 154/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0014 - acc: 0.9993 - val_loss: 0.0417 - val_acc: 0.9919\n",
      "Epoch 155/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0374 - val_acc: 0.9929\n",
      "Epoch 156/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0406 - val_acc: 0.9924\n",
      "Epoch 157/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0400 - val_acc: 0.9922\n",
      "Epoch 158/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0377 - val_acc: 0.9929\n",
      "Epoch 159/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0393 - val_acc: 0.9922\n",
      "Epoch 160/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0376 - val_acc: 0.9921\n",
      "Epoch 161/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0381 - val_acc: 0.9923\n",
      "Epoch 162/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0428 - val_acc: 0.9914\n",
      "Epoch 163/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0400 - val_acc: 0.9922\n",
      "Epoch 164/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0409 - val_acc: 0.9919\n",
      "Epoch 165/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0031 - acc: 0.9992 - val_loss: 0.0439 - val_acc: 0.9914\n",
      "Epoch 166/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0396 - val_acc: 0.9924\n",
      "Epoch 167/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0373 - val_acc: 0.9923\n",
      "Epoch 168/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0402 - val_acc: 0.9924\n",
      "Epoch 169/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0389 - val_acc: 0.9929\n",
      "Epoch 170/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0393 - val_acc: 0.9916\n",
      "Epoch 171/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0382 - val_acc: 0.9921\n",
      "Epoch 172/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0392 - val_acc: 0.9925\n",
      "Epoch 173/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0385 - val_acc: 0.9922\n",
      "Epoch 174/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0401 - val_acc: 0.9921\n",
      "Epoch 175/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0420 - val_acc: 0.9919\n",
      "Epoch 176/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0394 - val_acc: 0.9918\n",
      "Epoch 177/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0441 - val_acc: 0.9913\n",
      "Epoch 178/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0415 - val_acc: 0.9921\n",
      "Epoch 179/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0030 - acc: 0.9989 - val_loss: 0.0385 - val_acc: 0.9930\n",
      "Epoch 180/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0453 - val_acc: 0.9911\n",
      "Epoch 181/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0478 - val_acc: 0.9912\n",
      "Epoch 182/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0434 - val_acc: 0.9915\n",
      "Epoch 183/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0027 - acc: 0.9990 - val_loss: 0.0423 - val_acc: 0.9920\n",
      "Epoch 184/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0414 - val_acc: 0.9920\n",
      "Epoch 185/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0447 - val_acc: 0.9918\n",
      "Epoch 186/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0413 - val_acc: 0.9921\n",
      "Epoch 187/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0401 - val_acc: 0.9919\n",
      "Epoch 188/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0393 - val_acc: 0.9924\n",
      "Epoch 189/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0444 - val_acc: 0.9916\n",
      "Epoch 190/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0381 - val_acc: 0.9929\n",
      "Epoch 191/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0428 - val_acc: 0.9925\n",
      "Epoch 192/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0022 - acc: 0.9991 - val_loss: 0.0403 - val_acc: 0.9924\n",
      "Epoch 193/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0425 - val_acc: 0.9925\n",
      "Epoch 194/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0438 - val_acc: 0.9912\n",
      "Epoch 195/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0020 - acc: 0.9992 - val_loss: 0.0408 - val_acc: 0.9917\n",
      "Epoch 196/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0384 - val_acc: 0.9924\n",
      "Epoch 197/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0018 - acc: 0.9993 - val_loss: 0.0445 - val_acc: 0.9917\n",
      "Epoch 198/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0435 - val_acc: 0.9907\n",
      "Epoch 199/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0391 - val_acc: 0.9913\n",
      "Epoch 200/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0355 - val_acc: 0.9925\n",
      "Epoch 201/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0395 - val_acc: 0.9923\n",
      "Epoch 202/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0380 - val_acc: 0.9929\n",
      "Epoch 203/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0413 - val_acc: 0.9919\n",
      "Epoch 204/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0420 - val_acc: 0.9913\n",
      "Epoch 205/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0383 - val_acc: 0.9924\n",
      "Epoch 206/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0416 - val_acc: 0.9911\n",
      "Epoch 207/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0370 - val_acc: 0.9927\n",
      "Epoch 208/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0433 - val_acc: 0.9913\n",
      "Epoch 209/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0403 - val_acc: 0.9921\n",
      "Epoch 210/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0418 - val_acc: 0.9916\n",
      "Epoch 211/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0415 - val_acc: 0.9919\n",
      "Epoch 212/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0432 - val_acc: 0.9917\n",
      "Epoch 213/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0442 - val_acc: 0.9919\n",
      "Epoch 214/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0401 - val_acc: 0.9928\n",
      "Epoch 215/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0407 - val_acc: 0.9925\n",
      "Epoch 216/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0431 - val_acc: 0.9920\n",
      "Epoch 217/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0027 - acc: 0.9988 - val_loss: 0.0430 - val_acc: 0.9917\n",
      "Epoch 218/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9990 - val_loss: 0.0396 - val_acc: 0.9929\n",
      "Epoch 219/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0433 - val_acc: 0.9926\n",
      "Epoch 220/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0437 - val_acc: 0.9922\n",
      "Epoch 221/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0522 - val_acc: 0.9906\n",
      "Epoch 222/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0432 - val_acc: 0.9921\n",
      "Epoch 223/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0447 - val_acc: 0.9918\n",
      "Epoch 224/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0421 - val_acc: 0.9921\n",
      "Epoch 225/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0453 - val_acc: 0.9920\n",
      "Epoch 226/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0017 - acc: 0.9993 - val_loss: 0.0448 - val_acc: 0.9921\n",
      "Epoch 227/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0421 - val_acc: 0.9926\n",
      "Epoch 228/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0467 - val_acc: 0.9917\n",
      "Epoch 229/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0434 - val_acc: 0.9923\n",
      "Epoch 230/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0400 - val_acc: 0.9929\n",
      "Epoch 231/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0401 - val_acc: 0.9921\n",
      "Epoch 232/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0445 - val_acc: 0.9921\n",
      "Epoch 233/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0421 - val_acc: 0.9920\n",
      "Epoch 234/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0418 - val_acc: 0.9923\n",
      "Epoch 235/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0415 - val_acc: 0.9917\n",
      "Epoch 236/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0401 - val_acc: 0.9925\n",
      "Epoch 237/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0422 - val_acc: 0.9918\n",
      "Epoch 238/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0018 - acc: 0.9993 - val_loss: 0.0424 - val_acc: 0.9921\n",
      "Epoch 239/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0408 - val_acc: 0.9921\n",
      "Epoch 240/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0410 - val_acc: 0.9927\n",
      "Epoch 241/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0409 - val_acc: 0.9926\n",
      "Epoch 242/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0383 - val_acc: 0.9925\n",
      "Epoch 243/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0015 - acc: 0.9994 - val_loss: 0.0422 - val_acc: 0.9912\n",
      "Epoch 244/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0440 - val_acc: 0.9922\n",
      "Epoch 245/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0393 - val_acc: 0.9926\n",
      "Epoch 246/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0404 - val_acc: 0.9922\n",
      "Epoch 247/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0394 - val_acc: 0.9929\n",
      "Epoch 248/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0448 - val_acc: 0.9921\n",
      "Epoch 249/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0450 - val_acc: 0.9917\n",
      "Epoch 250/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0486 - val_acc: 0.9911\n",
      "Epoch 251/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0370 - val_acc: 0.9928\n",
      "Epoch 252/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0403 - val_acc: 0.9925\n",
      "Epoch 253/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0384 - val_acc: 0.9922\n",
      "Epoch 254/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0421 - val_acc: 0.9922\n",
      "Epoch 255/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0456 - val_acc: 0.9912\n",
      "Epoch 256/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0419 - val_acc: 0.9920\n",
      "Epoch 257/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0459 - val_acc: 0.9916\n",
      "Epoch 258/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0447 - val_acc: 0.9913\n",
      "Epoch 259/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0431 - val_acc: 0.9917\n",
      "Epoch 260/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0390 - val_acc: 0.9925\n",
      "Epoch 261/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0394 - val_acc: 0.9923\n",
      "Epoch 262/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0403 - val_acc: 0.9925\n",
      "Epoch 263/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0400 - val_acc: 0.9925\n",
      "Epoch 264/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.0387 - val_acc: 0.9928\n",
      "Epoch 265/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0403 - val_acc: 0.9917\n",
      "Epoch 266/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0416 - val_acc: 0.9925\n",
      "Epoch 267/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0423 - val_acc: 0.9927\n",
      "Epoch 268/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0389 - val_acc: 0.9921\n",
      "Epoch 269/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0365 - val_acc: 0.9931\n",
      "Epoch 270/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0425 - val_acc: 0.9924\n",
      "Epoch 271/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0381 - val_acc: 0.9922\n",
      "Epoch 272/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0411 - val_acc: 0.9931\n",
      "Epoch 273/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0420 - val_acc: 0.9921\n",
      "Epoch 274/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0471 - val_acc: 0.9916\n",
      "Epoch 275/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0378 - val_acc: 0.9925\n",
      "Epoch 276/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0400 - val_acc: 0.9926\n",
      "Epoch 277/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0012 - acc: 0.9995 - val_loss: 0.0429 - val_acc: 0.9924\n",
      "Epoch 278/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0015 - acc: 0.9994 - val_loss: 0.0388 - val_acc: 0.9929\n",
      "Epoch 279/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0401 - val_acc: 0.9926\n",
      "Epoch 280/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0391 - val_acc: 0.9923\n",
      "Epoch 281/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0404 - val_acc: 0.9921\n",
      "Epoch 282/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0024 - acc: 0.9988 - val_loss: 0.0383 - val_acc: 0.9924\n",
      "Epoch 283/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0018 - acc: 0.9993 - val_loss: 0.0390 - val_acc: 0.9920\n",
      "Epoch 284/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0018 - acc: 0.9993 - val_loss: 0.0415 - val_acc: 0.9917\n",
      "Epoch 285/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0421 - val_acc: 0.9917\n",
      "Epoch 286/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0427 - val_acc: 0.9923\n",
      "Epoch 287/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0423 - val_acc: 0.9920\n",
      "Epoch 288/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0024 - acc: 0.9990 - val_loss: 0.0412 - val_acc: 0.9915\n",
      "Epoch 289/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0377 - val_acc: 0.9931\n",
      "Epoch 290/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0454 - val_acc: 0.9915\n",
      "Epoch 291/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0437 - val_acc: 0.9916\n",
      "Epoch 292/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0388 - val_acc: 0.9925\n",
      "Epoch 293/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0374 - val_acc: 0.9926\n",
      "Epoch 294/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0440 - val_acc: 0.9914\n",
      "Epoch 295/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0363 - val_acc: 0.9933\n",
      "Epoch 296/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0399 - val_acc: 0.9926\n",
      "Epoch 297/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0482 - val_acc: 0.9914\n",
      "Epoch 298/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0427 - val_acc: 0.9918\n",
      "Epoch 299/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0403 - val_acc: 0.9924\n",
      "Epoch 300/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0420 - val_acc: 0.9925\n",
      "Epoch 301/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0364 - val_acc: 0.9929\n",
      "Epoch 302/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0388 - val_acc: 0.9925\n",
      "Epoch 303/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0391 - val_acc: 0.9929\n",
      "Epoch 304/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0442 - val_acc: 0.9922\n",
      "Epoch 305/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0410 - val_acc: 0.9918\n",
      "Epoch 306/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0427 - val_acc: 0.9921\n",
      "Epoch 307/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0401 - val_acc: 0.9926\n",
      "Epoch 308/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9994 - val_loss: 0.0429 - val_acc: 0.9921\n",
      "Epoch 309/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0398 - val_acc: 0.9927\n",
      "Epoch 310/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0434 - val_acc: 0.9925\n",
      "Epoch 311/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0408 - val_acc: 0.9925\n",
      "Epoch 312/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9993 - val_loss: 0.0411 - val_acc: 0.9921\n",
      "Epoch 313/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0439 - val_acc: 0.9926\n",
      "Epoch 314/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0477 - val_acc: 0.9911\n",
      "Epoch 315/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0392 - val_acc: 0.9928\n",
      "Epoch 316/400\n",
      "29399/29399 [==============================] - 7s - loss: 7.8116e-04 - acc: 0.9998 - val_loss: 0.0408 - val_acc: 0.9929\n",
      "Epoch 317/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0391 - val_acc: 0.9924\n",
      "Epoch 318/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0395 - val_acc: 0.9929\n",
      "Epoch 319/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0422 - val_acc: 0.9921\n",
      "Epoch 320/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0392 - val_acc: 0.9923\n",
      "Epoch 321/400\n",
      "29399/29399 [==============================] - 8s - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0417 - val_acc: 0.9921\n",
      "Epoch 322/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0380 - val_acc: 0.9930\n",
      "Epoch 323/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9992 - val_loss: 0.0416 - val_acc: 0.9921\n",
      "Epoch 324/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0411 - val_acc: 0.9925\n",
      "Epoch 325/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0018 - acc: 0.9993 - val_loss: 0.0471 - val_acc: 0.9920\n",
      "Epoch 326/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0491 - val_acc: 0.9916\n",
      "Epoch 327/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0032 - acc: 0.9988 - val_loss: 0.0488 - val_acc: 0.9911\n",
      "Epoch 328/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0022 - acc: 0.9991 - val_loss: 0.0392 - val_acc: 0.9926\n",
      "Epoch 329/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0018 - acc: 0.9993 - val_loss: 0.0461 - val_acc: 0.9917\n",
      "Epoch 330/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0419 - val_acc: 0.9914\n",
      "Epoch 331/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0376 - val_acc: 0.9925\n",
      "Epoch 332/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0441 - val_acc: 0.9920\n",
      "Epoch 333/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0380 - val_acc: 0.9928\n",
      "Epoch 334/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0414 - val_acc: 0.9927\n",
      "Epoch 335/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9993 - val_loss: 0.0423 - val_acc: 0.9925\n",
      "Epoch 336/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0396 - val_acc: 0.9932\n",
      "Epoch 337/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0405 - val_acc: 0.9927\n",
      "Epoch 338/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0443 - val_acc: 0.9917\n",
      "Epoch 339/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0408 - val_acc: 0.9925\n",
      "Epoch 340/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0454 - val_acc: 0.9921\n",
      "Epoch 341/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0389 - val_acc: 0.9930\n",
      "Epoch 342/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0412 - val_acc: 0.9920\n",
      "Epoch 343/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0439 - val_acc: 0.9925\n",
      "Epoch 344/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0408 - val_acc: 0.9930\n",
      "Epoch 345/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0422 - val_acc: 0.9922\n",
      "Epoch 346/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0017 - acc: 0.9992 - val_loss: 0.0443 - val_acc: 0.9928\n",
      "Epoch 347/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0435 - val_acc: 0.9925\n",
      "Epoch 348/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0433 - val_acc: 0.9927\n",
      "Epoch 349/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0480 - val_acc: 0.9922\n",
      "Epoch 350/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9991 - val_loss: 0.0393 - val_acc: 0.9929\n",
      "Epoch 351/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0407 - val_acc: 0.9924\n",
      "Epoch 352/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0407 - val_acc: 0.9922\n",
      "Epoch 353/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0417 - val_acc: 0.9919\n",
      "Epoch 354/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0410 - val_acc: 0.9927\n",
      "Epoch 355/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0439 - val_acc: 0.9921\n",
      "Epoch 356/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0430 - val_acc: 0.9925\n",
      "Epoch 357/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0449 - val_acc: 0.9922\n",
      "Epoch 358/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0011 - acc: 0.9995 - val_loss: 0.0409 - val_acc: 0.9927\n",
      "Epoch 359/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0442 - val_acc: 0.9926\n",
      "Epoch 360/400\n",
      "29399/29399 [==============================] - 6s - loss: 7.2703e-04 - acc: 0.9998 - val_loss: 0.0439 - val_acc: 0.9922\n",
      "Epoch 361/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0015 - acc: 0.9994 - val_loss: 0.0430 - val_acc: 0.9929\n",
      "Epoch 362/400\n",
      "29399/29399 [==============================] - 7s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0446 - val_acc: 0.9914\n",
      "Epoch 363/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0422 - val_acc: 0.9924\n",
      "Epoch 364/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0415 - val_acc: 0.9924\n",
      "Epoch 365/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0466 - val_acc: 0.9925\n",
      "Epoch 366/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9993 - val_loss: 0.0408 - val_acc: 0.9928\n",
      "Epoch 367/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0530 - val_acc: 0.9909\n",
      "Epoch 368/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0441 - val_acc: 0.9919\n",
      "Epoch 369/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0474 - val_acc: 0.9918\n",
      "Epoch 370/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0480 - val_acc: 0.9913\n",
      "Epoch 371/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0440 - val_acc: 0.9923\n",
      "Epoch 372/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0427 - val_acc: 0.9920\n",
      "Epoch 373/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0440 - val_acc: 0.9921\n",
      "Epoch 374/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0434 - val_acc: 0.9922\n",
      "Epoch 375/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0413 - val_acc: 0.9928\n",
      "Epoch 376/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0441 - val_acc: 0.9914\n",
      "Epoch 377/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0413 - val_acc: 0.9928\n",
      "Epoch 378/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0440 - val_acc: 0.9922\n",
      "Epoch 379/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0408 - val_acc: 0.9929\n",
      "Epoch 380/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0435 - val_acc: 0.9921\n",
      "Epoch 381/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0010 - acc: 0.9997 - val_loss: 0.0457 - val_acc: 0.9922\n",
      "Epoch 382/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0460 - val_acc: 0.9922\n",
      "Epoch 383/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0447 - val_acc: 0.9925\n",
      "Epoch 384/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9994 - val_loss: 0.0458 - val_acc: 0.9920\n",
      "Epoch 385/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0479 - val_acc: 0.9912\n",
      "Epoch 386/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0482 - val_acc: 0.9915\n",
      "Epoch 387/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0434 - val_acc: 0.9925\n",
      "Epoch 388/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0427 - val_acc: 0.9925\n",
      "Epoch 389/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0479 - val_acc: 0.9915\n",
      "Epoch 390/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0011 - acc: 0.9995 - val_loss: 0.0420 - val_acc: 0.9917\n",
      "Epoch 391/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0012 - acc: 0.9995 - val_loss: 0.0454 - val_acc: 0.9925\n",
      "Epoch 392/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0438 - val_acc: 0.9914\n",
      "Epoch 393/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0437 - val_acc: 0.9921\n",
      "Epoch 394/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0500 - val_acc: 0.9911\n",
      "Epoch 395/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0010 - acc: 0.9998 - val_loss: 0.0400 - val_acc: 0.9933\n",
      "Epoch 396/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0447 - val_acc: 0.9925\n",
      "Epoch 397/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9993 - val_loss: 0.0426 - val_acc: 0.9924\n",
      "Epoch 398/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0381 - val_acc: 0.9926\n",
      "Epoch 399/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0408 - val_acc: 0.9927\n",
      "Epoch 400/400\n",
      "29399/29399 [==============================] - 6s - loss: 0.0017 - acc: 0.9993 - val_loss: 0.0565 - val_acc: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xef199e8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn dl model\n",
    "ia_model.fit(X_train, y_labels, epochs=400, batch_size=2000, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the larger model\n",
    "def larger_model():\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/200\n",
      "33600/33600 [==============================] - 7s - loss: 2.0406 - acc: 0.3457 - val_loss: 1.3227 - val_acc: 0.7026\n",
      "Epoch 2/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.8587 - acc: 0.7436 - val_loss: 0.3894 - val_acc: 0.8767\n",
      "Epoch 3/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.3947 - acc: 0.8746 - val_loss: 0.2355 - val_acc: 0.9312\n",
      "Epoch 4/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.2611 - acc: 0.9214 - val_loss: 0.1716 - val_acc: 0.9486\n",
      "Epoch 5/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.2022 - acc: 0.9372 - val_loss: 0.1355 - val_acc: 0.9598\n",
      "Epoch 6/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.1656 - acc: 0.9496 - val_loss: 0.1112 - val_acc: 0.9685\n",
      "Epoch 7/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.1441 - acc: 0.9557 - val_loss: 0.0976 - val_acc: 0.9707\n",
      "Epoch 8/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.1283 - acc: 0.9605 - val_loss: 0.0865 - val_acc: 0.9732\n",
      "Epoch 9/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.1133 - acc: 0.9647 - val_loss: 0.0821 - val_acc: 0.9742\n",
      "Epoch 10/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.1054 - acc: 0.9677 - val_loss: 0.0747 - val_acc: 0.9763\n",
      "Epoch 11/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0947 - acc: 0.9711 - val_loss: 0.0704 - val_acc: 0.9781\n",
      "Epoch 12/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0901 - acc: 0.9721 - val_loss: 0.0652 - val_acc: 0.9787\n",
      "Epoch 13/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0842 - acc: 0.9745 - val_loss: 0.0610 - val_acc: 0.9806\n",
      "Epoch 14/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0793 - acc: 0.9765 - val_loss: 0.0596 - val_acc: 0.9798\n",
      "Epoch 15/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0739 - acc: 0.9772 - val_loss: 0.0551 - val_acc: 0.9808\n",
      "Epoch 16/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0709 - acc: 0.9784 - val_loss: 0.0531 - val_acc: 0.9830\n",
      "Epoch 17/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0672 - acc: 0.9793 - val_loss: 0.0544 - val_acc: 0.9813\n",
      "Epoch 18/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0617 - acc: 0.9806 - val_loss: 0.0501 - val_acc: 0.9826\n",
      "Epoch 19/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0610 - acc: 0.9814 - val_loss: 0.0469 - val_acc: 0.9842\n",
      "Epoch 20/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0575 - acc: 0.9819 - val_loss: 0.0467 - val_acc: 0.9835\n",
      "Epoch 21/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0568 - acc: 0.9821 - val_loss: 0.0443 - val_acc: 0.9850\n",
      "Epoch 22/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0531 - acc: 0.9834 - val_loss: 0.0445 - val_acc: 0.9836\n",
      "Epoch 23/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0521 - acc: 0.9836 - val_loss: 0.0420 - val_acc: 0.9860\n",
      "Epoch 24/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0495 - acc: 0.9851 - val_loss: 0.0439 - val_acc: 0.9848\n",
      "Epoch 25/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0477 - acc: 0.9854 - val_loss: 0.0395 - val_acc: 0.9858\n",
      "Epoch 26/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0448 - acc: 0.9863 - val_loss: 0.0395 - val_acc: 0.9869\n",
      "Epoch 27/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0462 - acc: 0.9854 - val_loss: 0.0400 - val_acc: 0.9865\n",
      "Epoch 28/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0425 - acc: 0.9863 - val_loss: 0.0396 - val_acc: 0.9856\n",
      "Epoch 29/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0423 - acc: 0.9867 - val_loss: 0.0372 - val_acc: 0.9870\n",
      "Epoch 30/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0402 - acc: 0.9870 - val_loss: 0.0394 - val_acc: 0.9873\n",
      "Epoch 31/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0406 - acc: 0.9864 - val_loss: 0.0377 - val_acc: 0.9864\n",
      "Epoch 32/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0381 - acc: 0.9884 - val_loss: 0.0367 - val_acc: 0.9881\n",
      "Epoch 33/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0372 - acc: 0.9879 - val_loss: 0.0362 - val_acc: 0.9876\n",
      "Epoch 34/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0347 - acc: 0.9887 - val_loss: 0.0388 - val_acc: 0.9865\n",
      "Epoch 35/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0352 - acc: 0.9882 - val_loss: 0.0380 - val_acc: 0.9867\n",
      "Epoch 36/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0346 - acc: 0.9888 - val_loss: 0.0365 - val_acc: 0.9873\n",
      "Epoch 37/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0335 - acc: 0.9891 - val_loss: 0.0349 - val_acc: 0.9887\n",
      "Epoch 38/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0340 - acc: 0.9888 - val_loss: 0.0354 - val_acc: 0.9869\n",
      "Epoch 39/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0329 - acc: 0.9892 - val_loss: 0.0350 - val_acc: 0.9885\n",
      "Epoch 40/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0309 - acc: 0.9899 - val_loss: 0.0363 - val_acc: 0.9876\n",
      "Epoch 41/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0295 - acc: 0.9902 - val_loss: 0.0368 - val_acc: 0.9876\n",
      "Epoch 42/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0281 - acc: 0.9908 - val_loss: 0.0347 - val_acc: 0.9880\n",
      "Epoch 43/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0280 - acc: 0.9908 - val_loss: 0.0348 - val_acc: 0.9893\n",
      "Epoch 44/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0269 - acc: 0.9913 - val_loss: 0.0348 - val_acc: 0.9882\n",
      "Epoch 45/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0266 - acc: 0.9913 - val_loss: 0.0333 - val_acc: 0.9892\n",
      "Epoch 46/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0246 - acc: 0.9921 - val_loss: 0.0334 - val_acc: 0.9894\n",
      "Epoch 47/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0262 - acc: 0.9913 - val_loss: 0.0329 - val_acc: 0.9899\n",
      "Epoch 48/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0248 - acc: 0.9922 - val_loss: 0.0334 - val_acc: 0.9890\n",
      "Epoch 49/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0246 - acc: 0.9913 - val_loss: 0.0337 - val_acc: 0.9893\n",
      "Epoch 50/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0234 - acc: 0.9922 - val_loss: 0.0336 - val_acc: 0.9895\n",
      "Epoch 51/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0237 - acc: 0.9921 - val_loss: 0.0320 - val_acc: 0.9895\n",
      "Epoch 52/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0230 - acc: 0.9925 - val_loss: 0.0336 - val_acc: 0.9890\n",
      "Epoch 53/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0210 - acc: 0.9930 - val_loss: 0.0324 - val_acc: 0.9892\n",
      "Epoch 54/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0221 - acc: 0.9934 - val_loss: 0.0328 - val_acc: 0.9892\n",
      "Epoch 55/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0218 - acc: 0.9928 - val_loss: 0.0320 - val_acc: 0.9895\n",
      "Epoch 56/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0225 - acc: 0.9932 - val_loss: 0.0336 - val_acc: 0.9898\n",
      "Epoch 57/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0209 - acc: 0.9930 - val_loss: 0.0341 - val_acc: 0.9892\n",
      "Epoch 58/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0200 - acc: 0.9936 - val_loss: 0.0322 - val_acc: 0.9894\n",
      "Epoch 59/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0186 - acc: 0.9942 - val_loss: 0.0323 - val_acc: 0.9904\n",
      "Epoch 60/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0195 - acc: 0.9937 - val_loss: 0.0348 - val_acc: 0.9894\n",
      "Epoch 61/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0182 - acc: 0.9942 - val_loss: 0.0321 - val_acc: 0.9901\n",
      "Epoch 62/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0190 - acc: 0.9938 - val_loss: 0.0322 - val_acc: 0.9906\n",
      "Epoch 63/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0191 - acc: 0.9937 - val_loss: 0.0322 - val_acc: 0.9902\n",
      "Epoch 64/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0181 - acc: 0.9938 - val_loss: 0.0327 - val_acc: 0.9908\n",
      "Epoch 65/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0179 - acc: 0.9935 - val_loss: 0.0314 - val_acc: 0.9908\n",
      "Epoch 66/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0166 - acc: 0.9946 - val_loss: 0.0313 - val_acc: 0.9911\n",
      "Epoch 67/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0154 - acc: 0.9952 - val_loss: 0.0311 - val_acc: 0.9913\n",
      "Epoch 68/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0156 - acc: 0.9950 - val_loss: 0.0326 - val_acc: 0.9905\n",
      "Epoch 69/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0160 - acc: 0.9946 - val_loss: 0.0320 - val_acc: 0.9908\n",
      "Epoch 70/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0153 - acc: 0.9947 - val_loss: 0.0320 - val_acc: 0.9914\n",
      "Epoch 71/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0158 - acc: 0.9944 - val_loss: 0.0327 - val_acc: 0.9899\n",
      "Epoch 72/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0153 - acc: 0.9947 - val_loss: 0.0326 - val_acc: 0.9906\n",
      "Epoch 73/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0153 - acc: 0.9947 - val_loss: 0.0335 - val_acc: 0.9902\n",
      "Epoch 74/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0135 - acc: 0.9958 - val_loss: 0.0313 - val_acc: 0.9917\n",
      "Epoch 75/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0155 - acc: 0.9948 - val_loss: 0.0334 - val_acc: 0.9907\n",
      "Epoch 76/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0141 - acc: 0.9954 - val_loss: 0.0335 - val_acc: 0.9915\n",
      "Epoch 77/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0126 - acc: 0.9958 - val_loss: 0.0335 - val_acc: 0.9905\n",
      "Epoch 78/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0129 - acc: 0.9960 - val_loss: 0.0338 - val_acc: 0.9899\n",
      "Epoch 79/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0135 - acc: 0.9960 - val_loss: 0.0336 - val_acc: 0.9913\n",
      "Epoch 80/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0126 - acc: 0.9958 - val_loss: 0.0342 - val_acc: 0.9901\n",
      "Epoch 81/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0133 - acc: 0.9956 - val_loss: 0.0353 - val_acc: 0.9904\n",
      "Epoch 82/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0145 - acc: 0.9953 - val_loss: 0.0317 - val_acc: 0.9917\n",
      "Epoch 83/200\n",
      "33600/33600 [==============================] - 6s - loss: 0.0147 - acc: 0.9954 - val_loss: 0.0351 - val_acc: 0.9896\n",
      "Epoch 84/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0123 - acc: 0.9952 - val_loss: 0.0363 - val_acc: 0.9899\n",
      "Epoch 85/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0128 - acc: 0.9955 - val_loss: 0.0351 - val_acc: 0.9901\n",
      "Epoch 86/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0125 - acc: 0.9957 - val_loss: 0.0340 - val_acc: 0.9901\n",
      "Epoch 87/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0114 - acc: 0.9963 - val_loss: 0.0343 - val_acc: 0.9900\n",
      "Epoch 88/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0119 - acc: 0.9964 - val_loss: 0.0347 - val_acc: 0.9911\n",
      "Epoch 89/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0119 - acc: 0.9960 - val_loss: 0.0351 - val_acc: 0.9910\n",
      "Epoch 90/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0118 - acc: 0.9957 - val_loss: 0.0361 - val_acc: 0.9896\n",
      "Epoch 91/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0104 - acc: 0.9964 - val_loss: 0.0350 - val_acc: 0.9907\n",
      "Epoch 92/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0114 - acc: 0.9962 - val_loss: 0.0356 - val_acc: 0.9907\n",
      "Epoch 93/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0102 - acc: 0.9964 - val_loss: 0.0370 - val_acc: 0.9906\n",
      "Epoch 94/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0099 - acc: 0.9968 - val_loss: 0.0361 - val_acc: 0.9910\n",
      "Epoch 95/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0107 - acc: 0.9966 - val_loss: 0.0341 - val_acc: 0.9911\n",
      "Epoch 96/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0112 - acc: 0.9964 - val_loss: 0.0347 - val_acc: 0.9917\n",
      "Epoch 97/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0101 - acc: 0.9967 - val_loss: 0.0375 - val_acc: 0.9912\n",
      "Epoch 98/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0101 - acc: 0.9966 - val_loss: 0.0363 - val_acc: 0.9905\n",
      "Epoch 99/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0105 - acc: 0.9965 - val_loss: 0.0368 - val_acc: 0.9911\n",
      "Epoch 100/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0335 - val_acc: 0.9911\n",
      "Epoch 101/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0091 - acc: 0.9972 - val_loss: 0.0347 - val_acc: 0.9917\n",
      "Epoch 102/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0106 - acc: 0.9964 - val_loss: 0.0359 - val_acc: 0.9917\n",
      "Epoch 103/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0106 - acc: 0.9963 - val_loss: 0.0363 - val_acc: 0.9908\n",
      "Epoch 104/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0101 - acc: 0.9966 - val_loss: 0.0380 - val_acc: 0.9906\n",
      "Epoch 105/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0090 - acc: 0.9971 - val_loss: 0.0380 - val_acc: 0.9902\n",
      "Epoch 106/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0090 - acc: 0.9968 - val_loss: 0.0339 - val_acc: 0.9915\n",
      "Epoch 107/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0087 - acc: 0.9972 - val_loss: 0.0367 - val_acc: 0.9908\n",
      "Epoch 108/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0085 - acc: 0.9972 - val_loss: 0.0348 - val_acc: 0.9915\n",
      "Epoch 109/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0079 - acc: 0.9974 - val_loss: 0.0382 - val_acc: 0.9910\n",
      "Epoch 110/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0085 - acc: 0.9971 - val_loss: 0.0377 - val_acc: 0.9913\n",
      "Epoch 111/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0094 - acc: 0.9971 - val_loss: 0.0419 - val_acc: 0.9887\n",
      "Epoch 112/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0092 - acc: 0.9969 - val_loss: 0.0380 - val_acc: 0.9905\n",
      "Epoch 113/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0388 - val_acc: 0.9908\n",
      "Epoch 114/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0336 - val_acc: 0.9914\n",
      "Epoch 115/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0077 - acc: 0.9976 - val_loss: 0.0356 - val_acc: 0.9915\n",
      "Epoch 116/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0082 - acc: 0.9974 - val_loss: 0.0364 - val_acc: 0.9914\n",
      "Epoch 117/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0380 - val_acc: 0.9899\n",
      "Epoch 118/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0074 - acc: 0.9975 - val_loss: 0.0378 - val_acc: 0.9913\n",
      "Epoch 119/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0362 - val_acc: 0.9919\n",
      "Epoch 120/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0369 - val_acc: 0.9915\n",
      "Epoch 121/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0075 - acc: 0.9977 - val_loss: 0.0366 - val_acc: 0.9906\n",
      "Epoch 122/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0073 - acc: 0.9978 - val_loss: 0.0395 - val_acc: 0.9906\n",
      "Epoch 123/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0068 - acc: 0.9978 - val_loss: 0.0373 - val_acc: 0.9904\n",
      "Epoch 124/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0072 - acc: 0.9978 - val_loss: 0.0370 - val_acc: 0.9917\n",
      "Epoch 125/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0379 - val_acc: 0.9907\n",
      "Epoch 126/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0075 - acc: 0.9975 - val_loss: 0.0367 - val_acc: 0.9911\n",
      "Epoch 127/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0393 - val_acc: 0.9907\n",
      "Epoch 128/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0077 - acc: 0.9975 - val_loss: 0.0378 - val_acc: 0.9919\n",
      "Epoch 129/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0355 - val_acc: 0.9917\n",
      "Epoch 130/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0059 - acc: 0.9982 - val_loss: 0.0368 - val_acc: 0.9923\n",
      "Epoch 131/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0075 - acc: 0.9976 - val_loss: 0.0370 - val_acc: 0.9917\n",
      "Epoch 132/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0368 - val_acc: 0.9904\n",
      "Epoch 133/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0072 - acc: 0.9975 - val_loss: 0.0374 - val_acc: 0.9917\n",
      "Epoch 134/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0368 - val_acc: 0.9910\n",
      "Epoch 135/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0075 - acc: 0.9974 - val_loss: 0.0360 - val_acc: 0.9917\n",
      "Epoch 136/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0062 - acc: 0.9980 - val_loss: 0.0395 - val_acc: 0.9908\n",
      "Epoch 137/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0078 - acc: 0.9973 - val_loss: 0.0368 - val_acc: 0.9912\n",
      "Epoch 138/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0069 - acc: 0.9980 - val_loss: 0.0369 - val_acc: 0.9912\n",
      "Epoch 139/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0062 - acc: 0.9981 - val_loss: 0.0355 - val_acc: 0.9921\n",
      "Epoch 140/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0386 - val_acc: 0.9914\n",
      "Epoch 141/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0066 - acc: 0.9979 - val_loss: 0.0361 - val_acc: 0.9927\n",
      "Epoch 142/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0058 - acc: 0.9983 - val_loss: 0.0402 - val_acc: 0.9917\n",
      "Epoch 143/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0056 - acc: 0.9982 - val_loss: 0.0409 - val_acc: 0.9900\n",
      "Epoch 144/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0067 - acc: 0.9977 - val_loss: 0.0365 - val_acc: 0.9910\n",
      "Epoch 145/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0059 - acc: 0.9982 - val_loss: 0.0382 - val_acc: 0.9908\n",
      "Epoch 146/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0408 - val_acc: 0.9905\n",
      "Epoch 147/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0386 - val_acc: 0.9914\n",
      "Epoch 148/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0078 - acc: 0.9971 - val_loss: 0.0386 - val_acc: 0.9910\n",
      "Epoch 149/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0370 - val_acc: 0.9914\n",
      "Epoch 150/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0070 - acc: 0.9978 - val_loss: 0.0382 - val_acc: 0.9908\n",
      "Epoch 151/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0378 - val_acc: 0.9918\n",
      "Epoch 152/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0394 - val_acc: 0.9910\n",
      "Epoch 153/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0395 - val_acc: 0.9915\n",
      "Epoch 154/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0379 - val_acc: 0.9919\n",
      "Epoch 155/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0355 - val_acc: 0.9911\n",
      "Epoch 156/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0067 - acc: 0.9977 - val_loss: 0.0364 - val_acc: 0.9915\n",
      "Epoch 157/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0402 - val_acc: 0.9917\n",
      "Epoch 158/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0399 - val_acc: 0.9904\n",
      "Epoch 159/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0405 - val_acc: 0.9910\n",
      "Epoch 160/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0419 - val_acc: 0.9905\n",
      "Epoch 161/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0394 - val_acc: 0.9914\n",
      "Epoch 162/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0408 - val_acc: 0.9911\n",
      "Epoch 163/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0071 - acc: 0.9974 - val_loss: 0.0441 - val_acc: 0.9913\n",
      "Epoch 164/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0069 - acc: 0.9979 - val_loss: 0.0415 - val_acc: 0.9907\n",
      "Epoch 165/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0404 - val_acc: 0.9913\n",
      "Epoch 166/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0414 - val_acc: 0.9914\n",
      "Epoch 167/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0046 - acc: 0.9989 - val_loss: 0.0412 - val_acc: 0.9908\n",
      "Epoch 168/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0394 - val_acc: 0.9913\n",
      "Epoch 169/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0380 - val_acc: 0.9920\n",
      "Epoch 170/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0429 - val_acc: 0.9902\n",
      "Epoch 171/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0055 - acc: 0.9985 - val_loss: 0.0391 - val_acc: 0.9913\n",
      "Epoch 172/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0060 - acc: 0.9981 - val_loss: 0.0405 - val_acc: 0.9913\n",
      "Epoch 173/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0423 - val_acc: 0.9919\n",
      "Epoch 174/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0416 - val_acc: 0.9915\n",
      "Epoch 175/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0050 - acc: 0.9986 - val_loss: 0.0414 - val_acc: 0.9910\n",
      "Epoch 176/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0051 - acc: 0.9981 - val_loss: 0.0405 - val_acc: 0.9918\n",
      "Epoch 177/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0420 - val_acc: 0.9918\n",
      "Epoch 178/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0455 - val_acc: 0.9901\n",
      "Epoch 179/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0421 - val_acc: 0.9911\n",
      "Epoch 180/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0406 - val_acc: 0.9912\n",
      "Epoch 181/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0388 - val_acc: 0.9913\n",
      "Epoch 182/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0410 - val_acc: 0.9914\n",
      "Epoch 183/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0050 - acc: 0.9981 - val_loss: 0.0404 - val_acc: 0.9918\n",
      "Epoch 184/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0403 - val_acc: 0.9914\n",
      "Epoch 185/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0428 - val_acc: 0.9911\n",
      "Epoch 186/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0422 - val_acc: 0.9910\n",
      "Epoch 187/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0392 - val_acc: 0.9915\n",
      "Epoch 188/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0400 - val_acc: 0.9910\n",
      "Epoch 189/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0437 - val_acc: 0.9900\n",
      "Epoch 190/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0401 - val_acc: 0.9913\n",
      "Epoch 191/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0420 - val_acc: 0.9914\n",
      "Epoch 192/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0051 - acc: 0.9981 - val_loss: 0.0447 - val_acc: 0.9910\n",
      "Epoch 193/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0430 - val_acc: 0.9908\n",
      "Epoch 194/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0408 - val_acc: 0.9913\n",
      "Epoch 195/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0430 - val_acc: 0.9913\n",
      "Epoch 196/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0044 - acc: 0.9983 - val_loss: 0.0415 - val_acc: 0.9911\n",
      "Epoch 197/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0049 - acc: 0.9985 - val_loss: 0.0419 - val_acc: 0.9907\n",
      "Epoch 198/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0439 - val_acc: 0.9907\n",
      "Epoch 199/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0418 - val_acc: 0.9917\n",
      "Epoch 200/200\n",
      "33600/33600 [==============================] - 5s - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0403 - val_acc: 0.9918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x49bca3c8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = larger_model()\n",
    "# learn dl model\n",
    "model.fit(X_train, y_labels, epochs=200, batch_size=2000, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model to local disk\n",
    "model.save('D:/deepL_google/project/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('D:/deepL_google/project/my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('D:/deepL_google/project/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = x_test.reshape(x_test.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this method will take model and numpy array data\n",
    "def get_submission_df(model, data):\n",
    "    np_prediction = model.predict_classes(data)\n",
    "    \n",
    "    df_submission = pd.DataFrame()\n",
    "    df_submission['ImageId'] = list(range(1,len(np_prediction)+1))\n",
    "    df_submission['Label'] = np_prediction\n",
    "    \n",
    "    return df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27904/28000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "df_submission = get_submission_df(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission.to_csv('D:/deepL_google/project/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with different conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(50, (7, 7), input_shape=(1, 28, 28), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(26, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(10, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29399 samples, validate on 12601 samples\n",
      "Epoch 1/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.3288 - acc: 0.8896 - val_loss: 0.1316 - val_acc: 0.9640\n",
      "Epoch 2/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.3265 - acc: 0.8872 - val_loss: 0.1265 - val_acc: 0.9651\n",
      "Epoch 3/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.3205 - acc: 0.8896 - val_loss: 0.1271 - val_acc: 0.9629\n",
      "Epoch 4/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.3077 - acc: 0.8940 - val_loss: 0.1217 - val_acc: 0.9653\n",
      "Epoch 5/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.3071 - acc: 0.8966 - val_loss: 0.1198 - val_acc: 0.9662\n",
      "Epoch 6/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.3028 - acc: 0.8966 - val_loss: 0.1170 - val_acc: 0.9666\n",
      "Epoch 7/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.2934 - acc: 0.9005 - val_loss: 0.1145 - val_acc: 0.9672\n",
      "Epoch 8/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.3000 - acc: 0.8989 - val_loss: 0.1152 - val_acc: 0.9667\n",
      "Epoch 9/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2923 - acc: 0.8985 - val_loss: 0.1120 - val_acc: 0.9679\n",
      "Epoch 10/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2842 - acc: 0.9046 - val_loss: 0.1079 - val_acc: 0.9685\n",
      "Epoch 11/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2842 - acc: 0.9051 - val_loss: 0.1078 - val_acc: 0.9699\n",
      "Epoch 12/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2813 - acc: 0.9052 - val_loss: 0.1088 - val_acc: 0.9686\n",
      "Epoch 13/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2771 - acc: 0.9055 - val_loss: 0.1069 - val_acc: 0.9680\n",
      "Epoch 14/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2759 - acc: 0.9061 - val_loss: 0.1070 - val_acc: 0.9686\n",
      "Epoch 15/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2693 - acc: 0.9081 - val_loss: 0.1038 - val_acc: 0.9706\n",
      "Epoch 16/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.2670 - acc: 0.9102 - val_loss: 0.1032 - val_acc: 0.9713\n",
      "Epoch 17/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.2585 - acc: 0.9125 - val_loss: 0.1011 - val_acc: 0.9712\n",
      "Epoch 18/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2658 - acc: 0.9107 - val_loss: 0.0997 - val_acc: 0.9706\n",
      "Epoch 19/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2618 - acc: 0.9123 - val_loss: 0.0997 - val_acc: 0.9708\n",
      "Epoch 20/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2570 - acc: 0.9137 - val_loss: 0.0990 - val_acc: 0.9712\n",
      "Epoch 21/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2524 - acc: 0.9148 - val_loss: 0.0971 - val_acc: 0.9719\n",
      "Epoch 22/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2493 - acc: 0.9166 - val_loss: 0.0956 - val_acc: 0.9720\n",
      "Epoch 23/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2504 - acc: 0.9148 - val_loss: 0.0968 - val_acc: 0.9723\n",
      "Epoch 24/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2422 - acc: 0.9179 - val_loss: 0.0921 - val_acc: 0.9729\n",
      "Epoch 25/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2456 - acc: 0.9173 - val_loss: 0.0917 - val_acc: 0.9732\n",
      "Epoch 26/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.2473 - acc: 0.9178 - val_loss: 0.0904 - val_acc: 0.9733\n",
      "Epoch 27/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2369 - acc: 0.9188 - val_loss: 0.0923 - val_acc: 0.9735\n",
      "Epoch 28/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.2433 - acc: 0.9187 - val_loss: 0.0916 - val_acc: 0.9738\n",
      "Epoch 29/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2369 - acc: 0.9200 - val_loss: 0.0901 - val_acc: 0.9737\n",
      "Epoch 30/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2385 - acc: 0.9187 - val_loss: 0.0900 - val_acc: 0.9734\n",
      "Epoch 31/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2350 - acc: 0.9210 - val_loss: 0.0898 - val_acc: 0.9744\n",
      "Epoch 32/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2316 - acc: 0.9222 - val_loss: 0.0903 - val_acc: 0.9739\n",
      "Epoch 33/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2318 - acc: 0.9216 - val_loss: 0.0865 - val_acc: 0.9747\n",
      "Epoch 34/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2280 - acc: 0.9237 - val_loss: 0.0848 - val_acc: 0.9747\n",
      "Epoch 35/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2259 - acc: 0.9244 - val_loss: 0.0870 - val_acc: 0.9745\n",
      "Epoch 36/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2269 - acc: 0.9231 - val_loss: 0.0860 - val_acc: 0.9758\n",
      "Epoch 37/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2264 - acc: 0.9218 - val_loss: 0.0840 - val_acc: 0.9752\n",
      "Epoch 38/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2249 - acc: 0.9228 - val_loss: 0.0831 - val_acc: 0.9758\n",
      "Epoch 39/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2206 - acc: 0.9268 - val_loss: 0.0823 - val_acc: 0.9755\n",
      "Epoch 40/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2178 - acc: 0.9259 - val_loss: 0.0827 - val_acc: 0.9758\n",
      "Epoch 41/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2182 - acc: 0.9254 - val_loss: 0.0804 - val_acc: 0.9756\n",
      "Epoch 42/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2128 - acc: 0.9286 - val_loss: 0.0820 - val_acc: 0.9760\n",
      "Epoch 43/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.2159 - acc: 0.9255 - val_loss: 0.0803 - val_acc: 0.9765\n",
      "Epoch 44/200\n",
      "29399/29399 [==============================] - 9s - loss: 0.2135 - acc: 0.9292 - val_loss: 0.0826 - val_acc: 0.9757\n",
      "Epoch 45/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2156 - acc: 0.9278 - val_loss: 0.0795 - val_acc: 0.9765\n",
      "Epoch 46/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2070 - acc: 0.9293 - val_loss: 0.0817 - val_acc: 0.9771\n",
      "Epoch 47/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2084 - acc: 0.9291 - val_loss: 0.0799 - val_acc: 0.9769\n",
      "Epoch 48/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2166 - acc: 0.9283 - val_loss: 0.0779 - val_acc: 0.9768\n",
      "Epoch 49/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2028 - acc: 0.9314 - val_loss: 0.0809 - val_acc: 0.9764\n",
      "Epoch 50/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2100 - acc: 0.9280 - val_loss: 0.0825 - val_acc: 0.9752\n",
      "Epoch 51/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2064 - acc: 0.9289 - val_loss: 0.0779 - val_acc: 0.9770\n",
      "Epoch 52/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1997 - acc: 0.9332 - val_loss: 0.0780 - val_acc: 0.9767\n",
      "Epoch 53/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2051 - acc: 0.9317 - val_loss: 0.0795 - val_acc: 0.9767\n",
      "Epoch 54/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2042 - acc: 0.9303 - val_loss: 0.0735 - val_acc: 0.9777\n",
      "Epoch 55/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2060 - acc: 0.9283 - val_loss: 0.0734 - val_acc: 0.9769\n",
      "Epoch 56/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.2025 - acc: 0.9318 - val_loss: 0.0754 - val_acc: 0.9774\n",
      "Epoch 57/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1982 - acc: 0.9318 - val_loss: 0.0735 - val_acc: 0.9779\n",
      "Epoch 58/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1985 - acc: 0.9319 - val_loss: 0.0773 - val_acc: 0.9773\n",
      "Epoch 59/200\n",
      "29399/29399 [==============================] - 10s - loss: 0.1974 - acc: 0.9333 - val_loss: 0.0787 - val_acc: 0.9767\n",
      "Epoch 60/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.2002 - acc: 0.9331 - val_loss: 0.0779 - val_acc: 0.9765\n",
      "Epoch 61/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1903 - acc: 0.9355 - val_loss: 0.0718 - val_acc: 0.9787\n",
      "Epoch 62/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1925 - acc: 0.9353 - val_loss: 0.0732 - val_acc: 0.9779\n",
      "Epoch 63/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1947 - acc: 0.9337 - val_loss: 0.0743 - val_acc: 0.9783\n",
      "Epoch 64/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1863 - acc: 0.9378 - val_loss: 0.0729 - val_acc: 0.9782\n",
      "Epoch 65/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1906 - acc: 0.9352 - val_loss: 0.0729 - val_acc: 0.9781\n",
      "Epoch 66/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1863 - acc: 0.9365 - val_loss: 0.0727 - val_acc: 0.9789\n",
      "Epoch 67/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1924 - acc: 0.9354 - val_loss: 0.0724 - val_acc: 0.9790\n",
      "Epoch 68/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1899 - acc: 0.9353 - val_loss: 0.0734 - val_acc: 0.9777\n",
      "Epoch 69/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1941 - acc: 0.9336 - val_loss: 0.0729 - val_acc: 0.9787\n",
      "Epoch 70/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1859 - acc: 0.9369 - val_loss: 0.0764 - val_acc: 0.9775\n",
      "Epoch 71/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1875 - acc: 0.9375 - val_loss: 0.0702 - val_acc: 0.9797\n",
      "Epoch 72/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1863 - acc: 0.9364 - val_loss: 0.0709 - val_acc: 0.9796\n",
      "Epoch 73/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1883 - acc: 0.9354 - val_loss: 0.0705 - val_acc: 0.9802\n",
      "Epoch 74/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1860 - acc: 0.9365 - val_loss: 0.0736 - val_acc: 0.9783\n",
      "Epoch 75/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1834 - acc: 0.9361 - val_loss: 0.0737 - val_acc: 0.9782\n",
      "Epoch 76/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1795 - acc: 0.9389 - val_loss: 0.0699 - val_acc: 0.9795\n",
      "Epoch 77/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1787 - acc: 0.9387 - val_loss: 0.0673 - val_acc: 0.9801\n",
      "Epoch 78/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1792 - acc: 0.9386 - val_loss: 0.0724 - val_acc: 0.9792\n",
      "Epoch 79/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1768 - acc: 0.9411 - val_loss: 0.0717 - val_acc: 0.9791\n",
      "Epoch 80/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1779 - acc: 0.9401 - val_loss: 0.0733 - val_acc: 0.9785\n",
      "Epoch 81/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1795 - acc: 0.9392 - val_loss: 0.0690 - val_acc: 0.9798\n",
      "Epoch 82/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1801 - acc: 0.9383 - val_loss: 0.0691 - val_acc: 0.9800\n",
      "Epoch 83/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1770 - acc: 0.9379 - val_loss: 0.0737 - val_acc: 0.9782\n",
      "Epoch 84/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1731 - acc: 0.9410 - val_loss: 0.0715 - val_acc: 0.9798\n",
      "Epoch 85/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1729 - acc: 0.9414 - val_loss: 0.0684 - val_acc: 0.9802\n",
      "Epoch 86/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1728 - acc: 0.9422 - val_loss: 0.0715 - val_acc: 0.9784\n",
      "Epoch 87/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1735 - acc: 0.9420 - val_loss: 0.0691 - val_acc: 0.9801\n",
      "Epoch 88/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1778 - acc: 0.9389 - val_loss: 0.0674 - val_acc: 0.9804\n",
      "Epoch 89/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1697 - acc: 0.9402 - val_loss: 0.0665 - val_acc: 0.9807\n",
      "Epoch 90/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1737 - acc: 0.9397 - val_loss: 0.0686 - val_acc: 0.9802\n",
      "Epoch 91/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1708 - acc: 0.9407 - val_loss: 0.0666 - val_acc: 0.9806\n",
      "Epoch 92/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1746 - acc: 0.9400 - val_loss: 0.0668 - val_acc: 0.9806\n",
      "Epoch 93/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1695 - acc: 0.9408 - val_loss: 0.0692 - val_acc: 0.9798\n",
      "Epoch 94/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1708 - acc: 0.9401 - val_loss: 0.0680 - val_acc: 0.9802\n",
      "Epoch 95/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1712 - acc: 0.9390 - val_loss: 0.0666 - val_acc: 0.9810\n",
      "Epoch 96/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1699 - acc: 0.9406 - val_loss: 0.0683 - val_acc: 0.9797\n",
      "Epoch 97/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1677 - acc: 0.9404 - val_loss: 0.0678 - val_acc: 0.9801\n",
      "Epoch 98/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1673 - acc: 0.9414 - val_loss: 0.0663 - val_acc: 0.9816\n",
      "Epoch 99/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1671 - acc: 0.9431 - val_loss: 0.0661 - val_acc: 0.9811\n",
      "Epoch 100/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1653 - acc: 0.9427 - val_loss: 0.0662 - val_acc: 0.9810\n",
      "Epoch 101/200\n",
      "29399/29399 [==============================] - 9s - loss: 0.1680 - acc: 0.9426 - val_loss: 0.0667 - val_acc: 0.9808\n",
      "Epoch 102/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1645 - acc: 0.9434 - val_loss: 0.0671 - val_acc: 0.9804\n",
      "Epoch 103/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1601 - acc: 0.9444 - val_loss: 0.0675 - val_acc: 0.9796\n",
      "Epoch 104/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1672 - acc: 0.9420 - val_loss: 0.0671 - val_acc: 0.9816\n",
      "Epoch 105/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1667 - acc: 0.9415 - val_loss: 0.0647 - val_acc: 0.9815\n",
      "Epoch 106/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1607 - acc: 0.9428 - val_loss: 0.0682 - val_acc: 0.9808\n",
      "Epoch 107/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1637 - acc: 0.9432 - val_loss: 0.0660 - val_acc: 0.9818\n",
      "Epoch 108/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1639 - acc: 0.9438 - val_loss: 0.0668 - val_acc: 0.9805\n",
      "Epoch 109/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1668 - acc: 0.9422 - val_loss: 0.0644 - val_acc: 0.9817\n",
      "Epoch 110/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1628 - acc: 0.9444 - val_loss: 0.0633 - val_acc: 0.9815\n",
      "Epoch 111/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1622 - acc: 0.9428 - val_loss: 0.0639 - val_acc: 0.9816\n",
      "Epoch 112/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1616 - acc: 0.9432 - val_loss: 0.0638 - val_acc: 0.9823\n",
      "Epoch 113/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1579 - acc: 0.9446 - val_loss: 0.0672 - val_acc: 0.9809\n",
      "Epoch 114/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1613 - acc: 0.9436 - val_loss: 0.0654 - val_acc: 0.9810\n",
      "Epoch 115/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1579 - acc: 0.9438 - val_loss: 0.0662 - val_acc: 0.9812\n",
      "Epoch 116/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1539 - acc: 0.9467 - val_loss: 0.0667 - val_acc: 0.9814\n",
      "Epoch 117/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1571 - acc: 0.9451 - val_loss: 0.0664 - val_acc: 0.9805\n",
      "Epoch 118/200\n",
      "29399/29399 [==============================] - 9s - loss: 0.1602 - acc: 0.9442 - val_loss: 0.0684 - val_acc: 0.9802\n",
      "Epoch 119/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1622 - acc: 0.9424 - val_loss: 0.0654 - val_acc: 0.9818\n",
      "Epoch 120/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1537 - acc: 0.9463 - val_loss: 0.0636 - val_acc: 0.9822\n",
      "Epoch 121/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1593 - acc: 0.9437 - val_loss: 0.0632 - val_acc: 0.9833\n",
      "Epoch 122/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1549 - acc: 0.9469 - val_loss: 0.0635 - val_acc: 0.9828\n",
      "Epoch 123/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1548 - acc: 0.9454 - val_loss: 0.0639 - val_acc: 0.9818\n",
      "Epoch 124/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1560 - acc: 0.9464 - val_loss: 0.0626 - val_acc: 0.9825\n",
      "Epoch 125/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1549 - acc: 0.9465 - val_loss: 0.0641 - val_acc: 0.9815\n",
      "Epoch 126/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1553 - acc: 0.9462 - val_loss: 0.0635 - val_acc: 0.9822\n",
      "Epoch 127/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1548 - acc: 0.9458 - val_loss: 0.0632 - val_acc: 0.9817\n",
      "Epoch 128/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1499 - acc: 0.9489 - val_loss: 0.0628 - val_acc: 0.9824\n",
      "Epoch 129/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1579 - acc: 0.9446 - val_loss: 0.0661 - val_acc: 0.9814\n",
      "Epoch 130/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1545 - acc: 0.9473 - val_loss: 0.0632 - val_acc: 0.9823\n",
      "Epoch 131/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1473 - acc: 0.9493 - val_loss: 0.0646 - val_acc: 0.9825\n",
      "Epoch 132/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1506 - acc: 0.9465 - val_loss: 0.0620 - val_acc: 0.9826\n",
      "Epoch 133/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1538 - acc: 0.9470 - val_loss: 0.0654 - val_acc: 0.9821\n",
      "Epoch 134/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1496 - acc: 0.9480 - val_loss: 0.0626 - val_acc: 0.9829\n",
      "Epoch 135/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1509 - acc: 0.9480 - val_loss: 0.0646 - val_acc: 0.9818\n",
      "Epoch 136/200\n",
      "29399/29399 [==============================] - 8s - loss: 0.1503 - acc: 0.9488 - val_loss: 0.0651 - val_acc: 0.9821\n",
      "Epoch 137/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1487 - acc: 0.9473 - val_loss: 0.0625 - val_acc: 0.9825\n",
      "Epoch 138/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1512 - acc: 0.9497 - val_loss: 0.0638 - val_acc: 0.9824\n",
      "Epoch 139/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1501 - acc: 0.9468 - val_loss: 0.0648 - val_acc: 0.9820\n",
      "Epoch 140/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1495 - acc: 0.9465 - val_loss: 0.0665 - val_acc: 0.9809\n",
      "Epoch 141/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1481 - acc: 0.9475 - val_loss: 0.0622 - val_acc: 0.9827\n",
      "Epoch 142/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1467 - acc: 0.9499 - val_loss: 0.0654 - val_acc: 0.9815\n",
      "Epoch 143/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1486 - acc: 0.9475 - val_loss: 0.0642 - val_acc: 0.9817\n",
      "Epoch 144/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1469 - acc: 0.9503 - val_loss: 0.0612 - val_acc: 0.9826\n",
      "Epoch 145/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1490 - acc: 0.9481 - val_loss: 0.0625 - val_acc: 0.9821\n",
      "Epoch 146/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1450 - acc: 0.9489 - val_loss: 0.0642 - val_acc: 0.9827\n",
      "Epoch 147/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1451 - acc: 0.9503 - val_loss: 0.0676 - val_acc: 0.9817\n",
      "Epoch 148/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1512 - acc: 0.9474 - val_loss: 0.0642 - val_acc: 0.9826\n",
      "Epoch 149/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1407 - acc: 0.9507 - val_loss: 0.0633 - val_acc: 0.9836\n",
      "Epoch 150/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1494 - acc: 0.9473 - val_loss: 0.0646 - val_acc: 0.9828\n",
      "Epoch 151/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1441 - acc: 0.9491 - val_loss: 0.0655 - val_acc: 0.9825\n",
      "Epoch 152/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1420 - acc: 0.9515 - val_loss: 0.0630 - val_acc: 0.9829\n",
      "Epoch 153/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1426 - acc: 0.9519 - val_loss: 0.0617 - val_acc: 0.9833\n",
      "Epoch 154/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1439 - acc: 0.9500 - val_loss: 0.0657 - val_acc: 0.9818\n",
      "Epoch 155/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1444 - acc: 0.9509 - val_loss: 0.0636 - val_acc: 0.9825\n",
      "Epoch 156/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1373 - acc: 0.9524 - val_loss: 0.0610 - val_acc: 0.9832\n",
      "Epoch 157/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1407 - acc: 0.9516 - val_loss: 0.0621 - val_acc: 0.9838\n",
      "Epoch 158/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1439 - acc: 0.9485 - val_loss: 0.0618 - val_acc: 0.9832\n",
      "Epoch 159/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1377 - acc: 0.9522 - val_loss: 0.0617 - val_acc: 0.9829\n",
      "Epoch 160/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1366 - acc: 0.9527 - val_loss: 0.0622 - val_acc: 0.9840\n",
      "Epoch 161/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1379 - acc: 0.9509 - val_loss: 0.0616 - val_acc: 0.9838\n",
      "Epoch 162/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1421 - acc: 0.9483 - val_loss: 0.0612 - val_acc: 0.9839\n",
      "Epoch 163/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1413 - acc: 0.9505 - val_loss: 0.0644 - val_acc: 0.9825\n",
      "Epoch 164/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1367 - acc: 0.9526 - val_loss: 0.0614 - val_acc: 0.9834\n",
      "Epoch 165/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1376 - acc: 0.9515 - val_loss: 0.0630 - val_acc: 0.9830\n",
      "Epoch 166/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1363 - acc: 0.9527 - val_loss: 0.0623 - val_acc: 0.9833\n",
      "Epoch 167/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1397 - acc: 0.9508 - val_loss: 0.0614 - val_acc: 0.9837\n",
      "Epoch 168/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1447 - acc: 0.9504 - val_loss: 0.0664 - val_acc: 0.9818\n",
      "Epoch 169/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1459 - acc: 0.9500 - val_loss: 0.0640 - val_acc: 0.9829\n",
      "Epoch 170/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1417 - acc: 0.9497 - val_loss: 0.0632 - val_acc: 0.9829\n",
      "Epoch 171/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1356 - acc: 0.9528 - val_loss: 0.0607 - val_acc: 0.9842\n",
      "Epoch 172/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1321 - acc: 0.9542 - val_loss: 0.0606 - val_acc: 0.9837\n",
      "Epoch 173/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1350 - acc: 0.9526 - val_loss: 0.0598 - val_acc: 0.9844\n",
      "Epoch 174/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1333 - acc: 0.9532 - val_loss: 0.0621 - val_acc: 0.9840\n",
      "Epoch 175/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1358 - acc: 0.9525 - val_loss: 0.0608 - val_acc: 0.9840\n",
      "Epoch 176/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1377 - acc: 0.9516 - val_loss: 0.0646 - val_acc: 0.9819\n",
      "Epoch 177/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1333 - acc: 0.9539 - val_loss: 0.0628 - val_acc: 0.9825\n",
      "Epoch 178/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1332 - acc: 0.9532 - val_loss: 0.0616 - val_acc: 0.9832\n",
      "Epoch 179/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1343 - acc: 0.9544 - val_loss: 0.0643 - val_acc: 0.9832\n",
      "Epoch 180/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1358 - acc: 0.9543 - val_loss: 0.0615 - val_acc: 0.9843\n",
      "Epoch 181/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1338 - acc: 0.9540 - val_loss: 0.0641 - val_acc: 0.9835\n",
      "Epoch 182/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1385 - acc: 0.9528 - val_loss: 0.0620 - val_acc: 0.9833\n",
      "Epoch 183/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1314 - acc: 0.9548 - val_loss: 0.0627 - val_acc: 0.9838\n",
      "Epoch 184/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1329 - acc: 0.9547 - val_loss: 0.0648 - val_acc: 0.9820\n",
      "Epoch 185/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1342 - acc: 0.9545 - val_loss: 0.0623 - val_acc: 0.9830\n",
      "Epoch 186/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1326 - acc: 0.9549 - val_loss: 0.0630 - val_acc: 0.9830\n",
      "Epoch 187/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1318 - acc: 0.9560 - val_loss: 0.0655 - val_acc: 0.9833\n",
      "Epoch 188/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1302 - acc: 0.9553 - val_loss: 0.0621 - val_acc: 0.9833\n",
      "Epoch 189/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1281 - acc: 0.9567 - val_loss: 0.0617 - val_acc: 0.9839\n",
      "Epoch 190/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1327 - acc: 0.9547 - val_loss: 0.0627 - val_acc: 0.9831\n",
      "Epoch 191/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1391 - acc: 0.9516 - val_loss: 0.0622 - val_acc: 0.9835\n",
      "Epoch 192/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1285 - acc: 0.9560 - val_loss: 0.0611 - val_acc: 0.9834\n",
      "Epoch 193/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1298 - acc: 0.9546 - val_loss: 0.0605 - val_acc: 0.9844\n",
      "Epoch 194/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1292 - acc: 0.9557 - val_loss: 0.0590 - val_acc: 0.9846\n",
      "Epoch 195/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1267 - acc: 0.9571 - val_loss: 0.0603 - val_acc: 0.9838\n",
      "Epoch 196/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1297 - acc: 0.9548 - val_loss: 0.0631 - val_acc: 0.9837\n",
      "Epoch 197/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1296 - acc: 0.9550 - val_loss: 0.0596 - val_acc: 0.9836\n",
      "Epoch 198/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1287 - acc: 0.9558 - val_loss: 0.0617 - val_acc: 0.9839\n",
      "Epoch 199/200\n",
      "29399/29399 [==============================] - 7s - loss: 0.1250 - acc: 0.9569 - val_loss: 0.0604 - val_acc: 0.9841\n",
      "Epoch 200/200\n",
      "29399/29399 [==============================] - 6s - loss: 0.1303 - acc: 0.9546 - val_loss: 0.0624 - val_acc: 0.9837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x715c37b8>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn dl model\n",
    "model.fit(X_train, y_labels, epochs=200, batch_size=2000, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
